{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cec276a-c1f8-4e55-b25f-ba4355e71477",
   "metadata": {},
   "source": [
    "## '''\n",
    "\n",
    "Lionel Nurweze, start: 20240925, last change: 2024-10-28\n",
    " \n",
    "lionel.nurweze -at- gmail -dot- com\n",
    "\n",
    "\n",
    "Repository: https://github.com/nurweze/reportCreator\n",
    "\n",
    "\n",
    "----------------------------------------------------------\n",
    "References: \n",
    "Refactored using chatGPT OpenAI. (2023). \n",
    "ChatGPT (GPT4o - Sep 2024 version) [Large language model].\n",
    "https://chat.openai.com/chat.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abde083-ef84-4da5-9cfe-c7894aab3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list\n",
    "#''' Might need to install packages below '''\n",
    "#!pip install pyreadr openpyxl pandas SQLAlchemy mysql-connector-python pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e353f9c-6a04-4bcf-8776-eab8b6f13cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a necessary directory\n",
    "\n",
    "#from tkinter import *\n",
    "#from tkinter.ttk import Notebook\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "'''\n",
    "The program uses a directory ReportTemplates where standard reports are saved. \n",
    "We will be saving a working file \"serialization_log.txt\" here too. \n",
    "This cell makes sure that directory exists and if it doesn't creates it.\n",
    "'''\n",
    "\n",
    "# Find out current working directory \n",
    "print(f\"Your working directory is for now {os.getcwd()}\")\n",
    "\n",
    "# Define the directory path\n",
    "log_directory = 'ReportTemplates'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Confirm that the directory ReportTemplates exists\n",
    "print(f\" '{log_directory}' directory is ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508681b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**0. Repository and requirements** \n",
    "\n",
    "- Jupyterlab: Requires python(tkinter) or ipython kernels.\n",
    "\n",
    "- Other required packages and their versions are specified in requirements.txt in the repository\n",
    "  as well as imported in each cell where they're used. \n",
    "\n",
    "\n",
    "**1. Problem statement:** \n",
    "\n",
    "In professional life, on a regular basis, you might need to create the same (or a similar) report using different data sources. The usual issue is that the different data sources have been created at different time periods by different persons/teams for different purposes and saved in different formats with different variable names. This is the usual data inconsinstency problem which demands a lot of work into reading in files, changing variable names to make them match, etc. One other major problem is also to be able to read in large datafiles with unknown values.\n",
    "\n",
    "If you are often producing the same report but from different data sources, it should be easy and pleasurable to produce the same report by making small adjustments. It should also be simple to create ad-hoc reports as well as manipulating data to produce the desired dataframe.\n",
    "\n",
    "\n",
    "**2. Idea:** \n",
    "\n",
    "Adress the above problem by easing the report creation with automated serialization routines, and do that with help of a GUI. The end-goal is that the GUI could be bundled in a stand-alone program (executable), for those who don't want to delve in the coding parts. It is also a measure of security if one does not wish to temper with already functioning code.  \n",
    "\n",
    "We use serialization to handle the different datatypes and sources. Serialization methods should solve the problem of handling large datasets.\n",
    "\n",
    "\n",
    "**3. Scope:** \n",
    " \n",
    "The project started as part of a course DD1334 - Database Technology, at KTH, Stockholm. One of the course goals is to be able to use serialization techniques which is implemented here. Another goal is the use of query languages to model and structure data according to given constraints. Within the scope of the course the aim was just to produce a standard report with example data.\n",
    "\n",
    "The author will continue to develop the program long-term and make it more stable and put it on a Github repository so that colleagues and others can reach it.\n",
    "The future goals are stated below in Continued developments. \n",
    "\n",
    "\n",
    " **4. Code architecture** \n",
    "\n",
    "The code exists in two formats that are both available on the Github repository: A Jupyter Notebook, and independent files. The latter is no longer being updated but the Jupyter Notebook follows the same structure, so that the new update code can be copied in the stand-alone files. \n",
    "\n",
    "The files functions are commented but the structure is basically the following: guimaker (.py) builds the GUI where the user sets the path to the datasource, the desired workspace and appropriate serialization method. importable (.py) identify the user inputs, identifies the file type, sets corresponding serialization method and is called by processes (.py) which contains the appropriate file-handling methods. serializable(.py) serializes or deserializes the data. exportable (.py) takes care of reading in the data and create the necessary report.  \n",
    "\n",
    "\n",
    "**5. Implementation**\n",
    "\n",
    "The program is able to handle input of .csv, .xlx/xls, JSON and SQL files and serialize them into .pickle or .json format and deserialize into a dataframe. \n",
    "\n",
    "Partly implemented are the handling of .xml, .r/.rdata and url:s. In order to handle URL files, for security reasons, you need to append your trusted source to a whitelist of sites.\n",
    "\n",
    "**6. Usage**\n",
    "\n",
    "In the GUI, the user adds a path to the datafile, confirms it, adds path to the workspace, confirms it, choose whether the file is going to be serialized with .pkl (Pickle) or with .json (JSON), and when that's done, the user presses the \"save data to workspace\" button. The data is then serialized in the right format (.pkl and/or .json) and with a filename that reminds of the original name as well as of the time of saving (while also making a serialization_log.txt). When it's for example files .csv or .xls files, the logic is simple, you read-in the file with the right method and serialize it (using both processes.py and importable.py). \n",
    "\n",
    "SQL methods are intricate so the decision was made to write a class of its own SQLHandler.py that handles those files. For now we're using a SQLite to handle MySQL/PostgreSQL databases syntaxes. Two alternatives were checked, the first one using SQLAlchemy to translate MySQL/PostgreSQL syntax to SQLite-readable logic but we've settled to using SQLAlchemy to point to built-in SQLite connections instead, which is a limiting factor in terms of which SQL syntax can be handled. Moreover, the logic for handling SQL files serialization is also different: We've decided to read-in the file as-is since there are already good packages (SQLite, SQLAlchemy) to handle those, running the SQL-program and serializing the results. \n",
    "\n",
    "There is a deserialization button, created in order to have an overview of the file since Pickle files are non-human readable. The user press the button and localize the .pkl file to deserialize.\n",
    "\n",
    "Since the whole project revolves around the ease of creating reports, a \"create report\" button has been added, which the user presses to access a directory with standards reports to run, in form of Python files. The user can either use those reports or just call the report0.py to make his own file and tailor it according to their needs. \n",
    "\n",
    "\n",
    "**7. Continued developments** \n",
    "\n",
    "- Improve overall functionality, mostly when it comes to the report-creation logic.\n",
    "- Implement SQLAlchemy which doesn't use SQLite engine to improve handling of more verbose MySQL/PostgreSQL files.  \n",
    "- Improve JSON serialization that's still lacking.\n",
    "- Have used pickle and json for serialization but should add a scrollbar to use other methods, python specific (parquet and feather seem promising) and others (probably  other javascript/json variants)\n",
    "- Include API handling too, e.g. World Bank's and Eurostat's APIs.\n",
    "- If code is regularly fetched from a website, implement data-scraping so that the report is generated automatically.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473de8e-5a5b-4584-9af3-3bb13e4fa7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "video_id = 'bx4t0ItftDI?si=WUDGuUONevz7on7V'  # Extracted from your YouTube link\n",
    "video_file_path = f'https://www.youtube.com/embed/{video_id}'\n",
    "HTML(f\"\"\"\n",
    "<iframe width=\"950\" height=\"800\" src=\"{video_file_path}\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2369e4c-d83c-4764-97a2-a1697e2ec68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "#from importable import FileReader, Serialization, Deserialization  # Adjust imports as necessary\n",
    "#from sql_handler import SQLHandler  # Ensure to import your SQLHandler\n",
    "\n",
    "# Configure logging for console\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class FileProcessor:\n",
    "\n",
    "    \n",
    "    def __init__(self, confirmed_paths: dict, selected_format: str = \"Pickle\"):\n",
    "        \n",
    "        self.datasources = confirmed_paths.get(\"datasources\", [])\n",
    "        self.workspace = confirmed_paths.get(\"workspace\")\n",
    "        self.selected_format = selected_format\n",
    "        self.file_reader = FileReader()\n",
    "        self.serializer = Serialization()\n",
    "        self.log_file_path = os.path.join(\"ReportTemplates\", \"serialization_log.txt\")\n",
    "        self.deserialization = Deserialization()\n",
    "\n",
    "    \n",
    "    def process_files(self) -> str:\n",
    "        \"\"\"\n",
    "        Process each data source and serialize the content.\n",
    "        \"\"\"\n",
    "        if not self.datasources or not self.workspace:\n",
    "            logging.error(\"No paths were provided for data sources or workspace.\")\n",
    "            return \"No paths were provided for data sources or workspace.\"\n",
    "    \n",
    "        result_messages = []\n",
    "    \n",
    "        for datasource in self.datasources:\n",
    "            logging.info(f\"Checking if data source exists: {datasource}\")\n",
    "    \n",
    "            if not os.path.isfile(datasource):\n",
    "                error_message = f\"Data source does not exist: {datasource}\"\n",
    "                logging.error(error_message)\n",
    "                result_messages.append(error_message)\n",
    "                continue\n",
    "    \n",
    "            # Log the type of file being processed\n",
    "            file_type = os.path.splitext(datasource)[1]\n",
    "            logging.info(f\"Processing file: {datasource} of type: {file_type}\")\n",
    "    \n",
    "            try:\n",
    "                logging.info(f\"Reading data from: {datasource}\")\n",
    "    \n",
    "                if datasource.endswith('.sql'):\n",
    "                    logging.info(f\"Detected SQL file: {datasource}\")\n",
    "                    sql_result_messages = self.process_SQL(datasource)\n",
    "                    result_messages.extend(sql_result_messages)\n",
    "                else:  # For non-SQL files, read and serialize \n",
    "                    data = self.file_reader.read_file(datasource)\n",
    "                    if data is not None:\n",
    "                        logging.info(f\"Data read successfully from: {datasource}\")\n",
    "                        saved_file_path = self.serialize_data(data, datasource)\n",
    "                        if saved_file_path:\n",
    "                            result_messages.append(f\"Serialized: {saved_file_path}\")\n",
    "                    else:\n",
    "                        error_message = f\"Failed to read data from: {datasource}\"\n",
    "                        logging.error(error_message)\n",
    "                        result_messages.append(error_message)\n",
    "    \n",
    "            except Exception as e:\n",
    "                error_message = f\"An error occurred while processing {datasource}: {e}\"\n",
    "                logging.error(error_message)\n",
    "                result_messages.append(error_message)\n",
    "    \n",
    "        return \"\\n\".join(result_messages)\n",
    "\n",
    "       \n",
    "\n",
    "    def process_SQL(self, datasource: str) -> list:\n",
    "        \"\"\"\n",
    "        Process and serialize the content of a SQL file.\n",
    "        \"\"\"\n",
    "        result_messages = []\n",
    "        logging.info(f\"Processing SQL file: {datasource}\")\n",
    "    \n",
    "        # Using SQLAlchemy's SQLite connection string format\n",
    "        self.connection_string = f\"sqlite:///{os.path.join(self.workspace, 'temp_database.db')}\"\n",
    "        logging.info(f\"Connecting to database with connection string: {self.connection_string}\")\n",
    "        \n",
    "        sql_handler = SQLHandler(workspace=self.workspace, serializer=self.serializer)\n",
    "    \n",
    "        try:\n",
    "            # Read SQL commands from the file\n",
    "            sql_commands = self.read_sql_file(datasource)\n",
    "            if not sql_commands:\n",
    "                error_message = f\"No valid SQL commands found in: {datasource}\"\n",
    "                logging.error(error_message)\n",
    "                return [error_message]\n",
    "    \n",
    "            for command in sql_commands:\n",
    "                command = command.strip()  # Clean up whitespace\n",
    "                if command:  # Skip empty commands\n",
    "                    logging.info(f\"Executing command: {command}\")\n",
    "                    try:\n",
    "                        self.sql_handler.execute_sql(command)  # Assuming this method executes the SQL command\n",
    "                    except Exception as e:\n",
    "                        error_message = f\"Error executing command '{command}': {e}\"\n",
    "                        logging.error(error_message)\n",
    "                        result_messages.append(error_message)\n",
    "    \n",
    "            # Serialize results after execution\n",
    "            saved_file_path = sql_handler.serialize_SQL(datasource, format=self.selected_format)\n",
    "            if saved_file_path:\n",
    "                result_messages.append(f\"Serialized SQL Result: {saved_file_path}\")\n",
    "            else:\n",
    "                error_message = f\"No results were serialized for: {datasource}\"\n",
    "                logging.warning(error_message)\n",
    "                result_messages.append(error_message)\n",
    "    \n",
    "        except Exception as e:\n",
    "            error_message = f\"Error processing SQL file {datasource}: {e}\"\n",
    "            logging.error(error_message)\n",
    "            result_messages.append(error_message)\n",
    "    \n",
    "        return result_messages\n",
    "\n",
    "\n",
    "    def read_sql_file(self, datasource: str) -> list:\n",
    "        \"\"\"Read and return the SQL commands from a file.\"\"\"\n",
    "        try:\n",
    "            with open(datasource, 'r') as file:\n",
    "                sql_commands = file.read().strip().split(';')  # Split by ';' to separate commands\n",
    "            return [cmd for cmd in sql_commands if cmd]  # Filter out empty commands\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to read SQL file {datasource}: {e}\")\n",
    "            return []\n",
    "\n",
    "    \n",
    "    def serialize_data(self, data, datasource: str) -> str:\n",
    "        \"\"\"Serialize the data and log the saved file path.\"\"\"\n",
    "        try:\n",
    "            saved_file_path = self.serializer.serialize(\n",
    "                data=data,\n",
    "                workspace=self.workspace,\n",
    "                format=self.selected_format,\n",
    "                original_filename=datasource\n",
    "            )\n",
    "            with open(self.log_file_path, 'a') as log_file:\n",
    "                log_file.write(f\"Serialized: {saved_file_path}\\n\")\n",
    "            logging.info(f\"Serialized: {saved_file_path}\")\n",
    "            return saved_file_path\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error serializing data from {datasource}: {e}\"\n",
    "            logging.error(error_message)\n",
    "            return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e812c6e-45e0-4218-9925-d91dce95a6d6",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE: On serialization of JSON and SQL files, and the file size problem.** \n",
    "\n",
    "1. For JSON files, there's usually no need to serialize them as byte/pickle-files to deserialize them afterwards, but that's also doable and could be practical if the user just needs to check the first lines of the file without opening the whole file. The user can just deserialize using the deserialize-button and the first lines are shown in console 2. When deserialized it's deserialized to JSON format again.\n",
    "\n",
    "2. With regards to SQL files, as stated above, the author recommends to serialize to pickle and read the files as-is, since there already exist packages that handle SQL querying (SQLite -used here- and SQLAlchemy), run the query using those packages and reseliaze the results if needed maybe to JSON or again to pickle. \n",
    "\n",
    "    For smaller sized datasets it could be practical to directly parse them as JSON, but it's not recommended for very large datasets (or if there are many files) since DBMS are considerably faster. DBMS store the relationsships (keys) between the different datasets so it might be futile to lose that information just for the sake of a JSON serialization. The user can try both methods to see what works better.\n",
    "\n",
    "\n",
    "3. Moreover, when it comes to SQL files, we're for now using the SQLite built-in package. The plan is that the code should be able to handle files with MySQL or PostgreSQL specific syntax and do that with the use of SQLAlchemy without the need to right own command. A temporarily method before full implementation is to use code translation, mapping MySQL/PostgreSQL specific syntaxes to code that functions with SQLite.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bb641-65c4-4188-a37f-38301abeb649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importable.py\n",
    "\n",
    "\n",
    "'''\n",
    "Contains the classes FileReader, Serialization and Deserialization. \n",
    "\n",
    "FileReader reads in the file depending on its original datastructure so that information about the datastructure\n",
    "is not lost under serialization.\n",
    "\n",
    "Serialization class serializes the files according to the serialization of choice, as a byte file \n",
    "with Python's pickle or as a readable JSON format. Next serialization modules to implement will be Feather and Parquet. \n",
    "\n",
    "Implemented?|  Filetype      |  Original datastructure |               Deserialized data structure \n",
    "            |                |                         |   Pickle (Always python byte)      |  JSON\n",
    "---------------------------------------------------------------------------------------------------------------\n",
    "    YES     |     CSV        | Tables (Rows/Column)    | List of Dictionaries or pandas DF  | Str\n",
    "----------------------------------------------------------------------------------------------------------------\n",
    "    YES     |    Excel       | Tables (Rows/Column)    | List of Dictionaries or pandas DF  | Dictionary\n",
    "----------------------------------------------------------------------------------------------------------------\n",
    "    YES     |    JSON        | Dict. or list of Dict.  |               -----                | Kept as original\n",
    "----------------------------------------------------------------------------------------------------------------\n",
    "  NOT YET   |    XML         | Tables (Rows/Column)    | List of Dictionaries or pandas DF  | \n",
    "----------------------------------------------------------------------------------------------------------------\n",
    "  NOT YET   |   R/RData      | Various (Vectors, Lists,| List of Dictionaries or pandas DF  |\n",
    "            |                     DataFrames)          |                                    | \n",
    "----------------------------------------------------------------------------------------------------------------\n",
    "   PARTLY   |    SQL (MySQL) |   MySQL code            | Keep as SQL code, use SQLite package |  .json (not fully implemented)     \n",
    "                                                        (recommended for very large files)    | \n",
    "----------------------------------------------------------------------------------------------------------------\n",
    "temporaly disabled |URL: http/https | see the different formats above         \n",
    "----------------------------------------------------------------------------------------------------------------\n",
    " \n",
    "'''\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import pyreadr\n",
    "import time\n",
    "import json\n",
    "import sqlite3\n",
    "import re\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "\n",
    "class FileReader:\n",
    "\n",
    "\n",
    "    def read_file(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Reads a file based on its extension and returns the data in an appropriate format.\n",
    "        \"\"\"\n",
    "        data = None  # Initialization \n",
    "    \n",
    "        # Getting the file extension\n",
    "        _, file_extension = os.path.splitext(file_path)\n",
    "        file_extension = file_extension.lower()\n",
    "    \n",
    "        # Check the file extension and use the appropriate reading method\n",
    "        try:\n",
    "            if file_extension == '.csv':\n",
    "                with open(file_path, 'r') as file:\n",
    "                    data = file.read()\n",
    "                logging.info(\"Original file is a .CSV File.\")\n",
    "                \n",
    "            elif file_extension == '.json':\n",
    "                with open(file_path, 'r') as file:\n",
    "                    data = json.load(file) \n",
    "                logging.info(\"Original file is a .JSON File.\")\n",
    "                         \n",
    "            elif file_extension in ['.xls', '.xlsx']:\n",
    "                data = pd.read_excel(file_path)  \n",
    "                logging.info(\"Original file is an Excel File (.xls/.xlsx).\")\n",
    "                \n",
    "            elif file_extension == '.xml':\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "                data = [{child.tag: child.text for child in elem} for elem in root]\n",
    "                logging.info(\"XML File Content as List of Dictionaries.\")                \n",
    "            \n",
    "            elif file_extension in ['.r', '.rdata']:\n",
    "                result = pyreadr.read_r(file_path)\n",
    "                data = result  # result is a dictionary-like object with dataframes\n",
    "                logging.info(\"R File Content in Dictionary Format.\")\n",
    "        \n",
    "            elif file_extension == '.sql':\n",
    "                # For SQL files, read the content as a string (can be processed later)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    data = file.read()\n",
    "                logging.info(\"Original file is a .SQL File.\")\n",
    "    \n",
    "            else:\n",
    "                logging.error(f\"Error: Unsupported file type: {file_extension}.\")\n",
    "                return None  # Indicate that the file type is unsupported\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading file {file_path}: {e}\")\n",
    "            return None  # Indicate that an error occurred\n",
    "    \n",
    "        return data\n",
    "\n",
    "\n",
    "class Serialization:\n",
    "\n",
    "    \n",
    "    def serialize(self, data, workspace, format, filename=None, original_filename=None):\n",
    "        \"\"\"\n",
    "        Serialize data to the specified format and save it to the workspace.\n",
    "        \"\"\"\n",
    "        # Validate the format before proceeding\n",
    "        format = format.lower()\n",
    "        valid_formats = {\"pkl\": \"pickle\", \"pickle\": \"pickle\", \"json\": \"json\"}\n",
    "        if format not in valid_formats:\n",
    "            raise ValueError(f\"Unsupported serialization format '{format}'. Please choose 'Pickle' or 'JSON'.\")\n",
    "\n",
    "        # Standardize format to correct case for file naming\n",
    "        format = valid_formats[format]\n",
    "\n",
    "        # Check if the workspace path exists\n",
    "        if not os.path.exists(workspace):\n",
    "            raise FileNotFoundError(f\"Workspace path '{workspace}' does not exist.\")\n",
    "    \n",
    "        # Generate filename if not provided\n",
    "        if not filename:\n",
    "            if original_filename:\n",
    "                base_name = os.path.splitext(os.path.basename(original_filename))[0]\n",
    "                extension = \"pkl\" if format == \"pickle\" else \"json\"\n",
    "                filename = f\"{base_name}.{extension}\"\n",
    "            else:\n",
    "                raise ValueError(\"Filename must be provided if original_filename is not given.\")\n",
    "    \n",
    "        file_path = os.path.join(workspace, filename)\n",
    "    \n",
    "        try:\n",
    "            # Serialize based on the selected format\n",
    "            if format == \"pickle\":\n",
    "                with open(file_path, 'wb') as pickled_file:\n",
    "                    pickle.dump(data, pickled_file)\n",
    "            elif format == \"json\":\n",
    "                with open(file_path, 'w') as json_file:\n",
    "                    json.dump(data, json_file, indent=4)\n",
    "            logging.info(f\"Data serialized successfully to {file_path}\")\n",
    "            return file_path  # Return only the path of the serialized file\n",
    "    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error serializing data to {file_path}: {e}\")\n",
    "            raise RuntimeError(f\"Error serializing data: {e}\")\n",
    "\n",
    "\n",
    "class Deserialization:\n",
    "    \n",
    "    '''\n",
    "    Just focuses on the deserialization according to right datastructure. \n",
    "    The processing of logging and checking files is done by processes.py\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, os_module=os):\n",
    "        self.os = os_module\n",
    "\n",
    "\n",
    "    def deserialize_data(self, file_path):\n",
    "        \"\"\"Deserialize data from a given file based on its extension.\"\"\"\n",
    "        if not self.os.path.isfile(file_path):\n",
    "            return f\"Error: The file does not exist: {file_path}\"\n",
    "    \n",
    "        _, file_extension = self.os.path.splitext(file_path)\n",
    "        file_extension = file_extension.lower()\n",
    "    \n",
    "        try:\n",
    "            if file_extension == '.pkl':\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    data = pickle.load(file)\n",
    "                    logging.info(f\"Deserialized data from {file_path}: {data}\")\n",
    "                    return data\n",
    "    \n",
    "            elif file_extension == '.json':\n",
    "                with open(file_path, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                    logging.info(f\"Deserialized JSON data from {file_path}: {data}\")\n",
    "                    return data\n",
    "    \n",
    "            else:\n",
    "                return f\"Error: Unsupported file type: {file_extension}. Please provide a .pkl or .json file.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error during deserialization: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a9697-d5c1-4624-a88d-7fe780c9261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLHandler.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sqlalchemy import create_engine, text\n",
    "import sqlite3\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "class SQLHandler:\n",
    "\n",
    "    \n",
    "    def __init__(self, workspace, serializer):\n",
    "        self.workspace = workspace\n",
    "        self.serializer = serializer\n",
    "        self.db_file_path = self._get_database_path()\n",
    "\n",
    "        self.delete_temp_database()\n",
    "        logging.info(f\"Setting database file path to: {self.db_file_path}\")\n",
    "        self.ensure_database_exists()\n",
    "        self.connection = sqlite3.connect(self.db_file_path)  # Direct SQLite connection\n",
    "        self.engine = create_engine(f'sqlite:///{self.db_file_path}')  # SQLAlchemy engine for SQLite\n",
    "        logging.info(f\"Connected to the database at {self.db_file_path}\")\n",
    "\n",
    "    \n",
    "    def _get_database_path(self):\n",
    "        return os.path.join(self.workspace, \"temp_database.db\")\n",
    "\n",
    "    \n",
    "    def delete_temp_database(self):\n",
    "        \"\"\"Delete the temporary database file if it exists.\"\"\"\n",
    "        if os.path.isfile(self.db_file_path):\n",
    "            os.remove(self.db_file_path)\n",
    "            logging.info(f\"Deleted existing database file: {self.db_file_path}\")\n",
    "\n",
    "            \n",
    "    def ensure_database_exists(self):\n",
    "        if not os.path.isfile(self.db_file_path):\n",
    "            try:\n",
    "                with sqlite3.connect(self.db_file_path) as conn:\n",
    "                    logging.info(f\"Database file created at {self.db_file_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to create database file: {e}\")\n",
    "                raise RuntimeError(f\"Failed to create database file: {e}\")\n",
    "        else:\n",
    "            logging.info(f\"Database file already exists at {self.db_file_path}\")\n",
    "\n",
    "    \n",
    "    def read_sql(self, file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                sql_commands = file.read()\n",
    "                logging.info(f\"Read SQL commands: {sql_commands}\")  # Log the read commands\n",
    "                return sql_commands\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error reading SQL file: {e}\")\n",
    "            raise RuntimeError(f\"Error reading SQL file: {e}\")\n",
    "\n",
    "\n",
    "    def identify_dialect(self, sql_commands):\n",
    "        if \"AUTO_INCREMENT\" in sql_commands or \"CREATE TABLE\" in sql_commands:\n",
    "            return 'mysql'\n",
    "        elif \"SERIAL\" in sql_commands:\n",
    "            return 'postgresql'\n",
    "        return 'sqlite'  # Default to SQLite\n",
    "\n",
    "    \n",
    "    def _execute_with_sqlite(self, sql_commands):\n",
    "        try:\n",
    "            commands = sql_commands.strip().split(';')\n",
    "            for command in commands:\n",
    "                command = command.strip()\n",
    "                if command:\n",
    "                    logging.info(f\"Executing SQL command: {command}\")\n",
    "                    result = self.connection.execute(command)\n",
    "                    logging.info(f\"Executed SQL command: {command}. Affected rows: {result.rowcount}\")  \n",
    "            self.connection.commit()  \n",
    "            logging.info(\"All changes committed to the database.\")  \n",
    "        except Exception as e:\n",
    "            logging.error(f\"SQLite execution failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def execute_sql(self, sql_commands):\n",
    "        dialect = self.identify_dialect(sql_commands)\n",
    "        logging.info(f\"Identified SQL dialect: {dialect}\")\n",
    "        \n",
    "        try:\n",
    "            if dialect == 'sqlite':\n",
    "                logging.info(\"Executing SQL commands in SQLite dialect.\")\n",
    "                self._execute_with_sqlite(sql_commands)\n",
    "            else:\n",
    "                logging.warning(f\"Dialect {dialect} not supported; executing with SQLite as a fallback.\")\n",
    "                self._execute_with_sqlite(sql_commands)  # Fallback to SQLite\n",
    "    \n",
    "            # Return a success message if execution reaches here\n",
    "            return \"SQL commands executed successfully.\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log the error and return an error message\n",
    "            logging.error(f\"SQL execution failed: {e}\")\n",
    "            return f\"Error executing SQL commands: {e}\"\n",
    "\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        try:\n",
    "            results = {}\n",
    "            table_names = self._get_all_table_names()\n",
    "            if not table_names:\n",
    "                logging.warning(\"No tables found in the database to fetch data from.\")\n",
    "                return results\n",
    "            \n",
    "            logging.info(f\"Fetching data from tables: {table_names}\")\n",
    "            for (table_name,) in table_names:\n",
    "                try:\n",
    "                    results[table_name] = pd.read_sql_table(table_name, con=self.engine)\n",
    "                    logging.info(f\"Fetched data from '{table_name}': {results[table_name]}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error reading data from table '{table_name}': {e}\") \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching data: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def _get_all_table_names(self):\n",
    "        return self.connection.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "\n",
    "    \n",
    "    def serialize_SQL(self, datasource: str, format: str) -> str:\n",
    "        \"\"\"Execute SQL commands from a file and serialize the results.\"\"\"\n",
    "        result_messages = []\n",
    "        \n",
    "        try:\n",
    "            with open(datasource, 'r') as sql_file:\n",
    "                sql_commands = sql_file.read()\n",
    "            \n",
    "            self.execute_sql(sql_commands)  # Execute SQL commands\n",
    "            \n",
    "            # Fetch all data after execution\n",
    "            results = self.get_all_data()  # Ensure this returns a valid DataFrame\n",
    "            logging.info(f\"Results after executing SQL: {results}\")  # Log fetched results\n",
    "            \n",
    "            if results is not None and not results.empty:\n",
    "                saved_file_path = self.serializer.serialize(\n",
    "                    data=results,\n",
    "                    workspace=self.workspace,\n",
    "                    format=format,\n",
    "                    original_filename=datasource\n",
    "                )\n",
    "                result_messages.append(f\"Serialized SQL Result: {saved_file_path}\")\n",
    "            else:\n",
    "                result_messages.append(\"No results to serialize from executed SQL.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in serialize_SQL: {e}\")\n",
    "            result_messages.append(f\"Error processing SQL file {datasource}: {e}\")\n",
    "    \n",
    "        return result_messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bfb301-7cc4-4db6-be21-e4379e01d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GUIMaker.py\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "#import importable\n",
    "\n",
    "#from SQLHandler import SQLHandler\n",
    "\n",
    "class GUIMaker:\n",
    "\n",
    "    \n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        self.master.title(\"Data Processor\")\n",
    "        self.data_path = \"\"\n",
    "        self.workspace_path = \"\"\n",
    "        self.serialization_format = tk.StringVar(value=\"Pickle\")  # Default to Pickle format\n",
    "        self.sql_handler = SQLHandler()\n",
    "        self.file_reader = importable.FileReader()  # Instance of FileReader\n",
    "        self.serializer = importable.Serialization()  # Instance of Serialization\n",
    "\n",
    "        # Setup GUI layout\n",
    "        self.setup_gui()\n",
    "\n",
    "    \n",
    "    def setup_gui(self):\n",
    "        # Path Entry for Data Source\n",
    "        tk.Label(self.master, text=\"Data Source Path:\").grid(row=0, column=0, sticky=\"e\")\n",
    "        self.data_entry = tk.Entry(self.master, width=50)\n",
    "        self.data_entry.grid(row=0, column=1)\n",
    "        tk.Button(self.master, text=\"Browse\", command=self.browse_data).grid(row=0, column=2)\n",
    "\n",
    "        # Path Entry for Workspace\n",
    "        tk.Label(self.master, text=\"Workspace Path:\").grid(row=1, column=0, sticky=\"e\")\n",
    "        self.workspace_entry = tk.Entry(self.master, width=50)\n",
    "        self.workspace_entry.grid(row=1, column=1)\n",
    "        tk.Button(self.master, text=\"Browse\", command=self.browse_workspace).grid(row=1, column=2)\n",
    "\n",
    "        # Serialization Format Option\n",
    "        tk.Label(self.master, text=\"Select Serialization Format:\").grid(row=2, column=0, sticky=\"e\")\n",
    "        format_options = [\"Pickle\", \"JSON\"]\n",
    "        self.format_menu = tk.OptionMenu(self.master, self.serialization_format, *format_options)\n",
    "        self.format_menu.grid(row=2, column=1, sticky=\"w\")\n",
    "\n",
    "        # Save Data Button\n",
    "        tk.Button(self.master, text=\"Save Data\", command=self.save_data_to_workspace).grid(row=3, column=1, pady=10)\n",
    "\n",
    "    \n",
    "    def browse_data(self):\n",
    "        file_path = filedialog.askopenfilename(title=\"Select Data File\")\n",
    "        if file_path:\n",
    "            self.data_entry.delete(0, tk.END)\n",
    "            self.data_entry.insert(0, file_path)\n",
    "            self.data_path = file_path\n",
    "\n",
    "    \n",
    "    def browse_workspace(self):\n",
    "        directory = filedialog.askdirectory(title=\"Select Workspace Directory\")\n",
    "        if directory:\n",
    "            self.workspace_entry.delete(0, tk.END)\n",
    "            self.workspace_entry.insert(0, directory)\n",
    "            self.workspace_path = directory\n",
    "\n",
    "    \n",
    "    def save_data_to_workspace(self):\n",
    "        # Retrieve paths and format selection\n",
    "        self.data_path = self.data_entry.get()\n",
    "        self.workspace_path = self.workspace_entry.get()\n",
    "        format_choice = self.serialization_format.get().lower()\n",
    "    \n",
    "        # Input validation\n",
    "        if not self.data_path or not self.workspace_path:\n",
    "            messagebox.showerror(\"Error\", \"Please specify both data source and workspace paths.\")\n",
    "            return\n",
    "    \n",
    "        # Determine original filename and log filename for confirmation\n",
    "        original_filename = os.path.basename(self.data_path)\n",
    "        log_filename = f\"{os.path.splitext(original_filename)[0]}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "        # Process the file based on type\n",
    "        _, file_extension = os.path.splitext(self.data_path)\n",
    "        file_extension = file_extension.lower()\n",
    "    \n",
    "        try:\n",
    "            if file_extension == \".sql\":\n",
    "                result_messages = []\n",
    "                if \"mysql\" in self.data_path.lower():\n",
    "                    result_messages = self.sql_handler.process_sqlalchemy(self.data_path)\n",
    "                else:\n",
    "                    result_messages = self.sql_handler.process_sqlite(self.data_path)\n",
    "    \n",
    "                # Debugging log\n",
    "                logging.info(f\"SQL processing results: {result_messages}\")\n",
    "    \n",
    "                # Check for success message in results\n",
    "                if not result_messages or any(\"Error\" in msg for msg in result_messages):\n",
    "                    messagebox.showerror(\"Error\", \"Failed to process SQL file. \" + \"\\n\".join(result_messages))\n",
    "                    return\n",
    "                \n",
    "                # Serialize SQL result\n",
    "                serialized_file_path = self.serializer.serialize(result_messages, self.workspace_path, format_choice,\n",
    "                                                                 filename=f\"{log_filename}.{format_choice}\",\n",
    "                                                                 original_filename=original_filename)\n",
    "    \n",
    "            else:\n",
    "                # Process non-SQL files\n",
    "                data = self.file_reader.read_file(self.data_path)\n",
    "                serialized_file_path = self.serializer.serialize(data, self.workspace_path, format_choice,\n",
    "                                                                 filename=f\"{log_filename}.{format_choice}\",\n",
    "                                                                 original_filename=original_filename)\n",
    "    \n",
    "            messagebox.showinfo(\"Success\", f\"Data saved to {serialized_file_path}\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to save data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddddae-8e46-42bb-9b31-b31d27b4ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exportable.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tkinter import filedialog\n",
    "'''\n",
    "import Serialization, Deserialization\n",
    "import FileProcessor\n",
    "'''\n",
    "\n",
    "''' \n",
    "Important note: that the ReportTemplates directory is hardcoded as \n",
    "the default map for all reports. \n",
    "'''\n",
    "\n",
    "class Exportable: \n",
    "\n",
    "\n",
    "    def __init__(self, gui = None):\n",
    "        self.gui = gui  # Reference to the GUIMaker instance\n",
    "        self.log_file_path = os.path.join(\"ReportTemplates\", \"serialization_log.txt\")\n",
    "        \n",
    "\n",
    "    def create_report(self):\n",
    "        \"\"\"Open a Python file for editing and running.\"\"\"\n",
    "        file_path = self.get_python_file_path()  # Open file dialog to get the path of the report\n",
    "        if not file_path:\n",
    "            self.gui.log_to_console(\"Error: No file path provided for the report.\")\n",
    "            return\n",
    "    \n",
    "        with open(file_path, 'r') as file:\n",
    "            report_code = file.read()\n",
    "    \n",
    "        # Open a new window for editing the Python script\n",
    "        editor_window = tk.Toplevel(self.gui.window)\n",
    "        editor_window.title(f\"Edit Report - {os.path.basename(file_path)}\")  # Display the file name in the window title\n",
    "    \n",
    "        # Create a text editor widget to display the content of the Python report\n",
    "        editor_text = tk.Text(editor_window, height=20, width=80)\n",
    "        editor_text.insert(tk.END, report_code)  # Insert the contents of the report\n",
    "        editor_text.pack()\n",
    "    \n",
    "        # Define a button to run the report, passing the file path to run_report\n",
    "        run_button = tk.Button(\n",
    "            editor_window,\n",
    "            text=\"Run Report\",\n",
    "            command=lambda: self.run_report(editor_text.get(\"1.0\", tk.END), report_file_path=file_path)\n",
    "        )\n",
    "        run_button.pack()\n",
    "\n",
    "\n",
    "    def get_python_file_path(self):\n",
    "        \"\"\"Open a file dialog to select a Python file.\"\"\"\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"Select a Python file\",\n",
    "            filetypes=[(\"Python files\", \"*.py\"), (\"All files\", \"*.*\")]\n",
    "        )\n",
    "        return file_path\n",
    "\n",
    "\n",
    "    def run_report(self, report_code, report_file_path=None):\n",
    "        \"\"\"Execute the report code with the proper file path and log messages to console 1 and output to console 2.\"\"\"\n",
    "        output_buffer = io.StringIO()\n",
    "        error_buffer = io.StringIO()\n",
    "        sys.stdout = output_buffer  # Redirect stdout to capture print statements\n",
    "        sys.stderr = error_buffer   # Redirect stderr to capture errors\n",
    "    \n",
    "        # Ensure ReportTemplates is added to sys.path only once\n",
    "        report_dir = os.path.join(os.path.dirname(__file__), \"ReportTemplates\")\n",
    "        if report_dir not in sys.path:\n",
    "            sys.path.append(report_dir)\n",
    "    \n",
    "        # Change directory to ReportTemplates\n",
    "        os.chdir(report_dir)\n",
    "    \n",
    "        # Create a custom globals dictionary with __file__ set to the provided report file path\n",
    "        exec_globals = {\"__file__\": report_file_path} if report_file_path else {}\n",
    "    \n",
    "        try:\n",
    "            # Run the provided report code with the modified globals context\n",
    "            exec(report_code, exec_globals)\n",
    "    \n",
    "            # Capture the standard output and error output\n",
    "            output = output_buffer.getvalue()\n",
    "            error_output = error_buffer.getvalue()\n",
    "    \n",
    "            # Log outputs accordingly\n",
    "            if output:\n",
    "                self.gui.log_to_console_2(output)  # Log stdout to console 2\n",
    "            if error_output:\n",
    "                self.gui.log_to_console(f\"Errors detected:\\n{error_output}\")  # Log stderr to console 1\n",
    "    \n",
    "            # Handle cases where no output is produced\n",
    "            if not output and not error_output:\n",
    "                self.gui.log_to_console_2(\"No output produced.\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            # Capture unexpected exceptions in a more readable format\n",
    "            self.gui.log_to_console(f\"Error executing report: {e}\")\n",
    "        finally:\n",
    "            # Reset stdout and stderr back to their original streams\n",
    "            sys.stdout = sys.__stdout__\n",
    "            sys.stderr = sys.__stderr__\n",
    "\n",
    "\n",
    "    def read_serialization_log(self):\n",
    "        \"\"\"Read the serialization log and return the list of serialized or deserialized file paths.\"\"\"\n",
    "        log_file_path = os.path.join(os.path.dirname(__file__), \"ReportTemplates\", \"serialization_log.txt\")\n",
    "\n",
    "        if not os.path.isfile(log_file_path):\n",
    "            logging.error(\"Serialization log does not exist.\")\n",
    "            return []\n",
    "\n",
    "        # Debugging: Print the log file path\n",
    "        print(f\"Reading log file at: {log_file_path}\")\n",
    "\n",
    "        with open(log_file_path, 'r') as log_file:\n",
    "            lines = log_file.readlines()\n",
    "\n",
    "        # Print log file contents for debugging\n",
    "        print(\"Log file contents:\")\n",
    "        for line in lines:\n",
    "            print(line.strip())\n",
    "\n",
    "        # Extract file paths from log\n",
    "        file_paths = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if \"Serialized:\" in line or \"Deserialized:\" in line:\n",
    "                parts = line.split(\":\")\n",
    "                if len(parts) > 1:\n",
    "                    file_path = parts[1].strip()\n",
    "                    file_paths.append(file_path)\n",
    "                    print(f\"Extracted file path: '{file_path}'\")\n",
    "\n",
    "        if not file_paths:\n",
    "            logging.error(\"No file paths found in the serialization log.\")\n",
    "        \n",
    "        return file_paths\n",
    "\n",
    "\n",
    "    def log_deserialization(self, file_path):\n",
    "        \"\"\"Log the deserialized file path to the serialization log.\"\"\"\n",
    "        with open(self.log_file_path, 'a') as log_file:\n",
    "            log_file.write(f\"Deserialized: {file_path}\\n\")\n",
    "\n",
    "\n",
    "class ReportGenerator:\n",
    "    \n",
    "    '''\n",
    "    Contains methods that collect the datapaths of processed files. \n",
    "    Is called by report0.py to produce standard report. \n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger()\n",
    "        self.deserializer = Deserialization()  # Initialize once\n",
    "        self.current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "    \n",
    "    def generate_report(self):\n",
    "        self.logger.info(\"Starting report generation...\")\n",
    "        self.logger.info(f\"Current directory: {self.current_dir}\")\n",
    "    \n",
    "        # Initialize Exportable to read the serialization log\n",
    "        exportable_instance = Exportable()\n",
    "        log_file_path = os.path.join(self.current_dir, \"serialization_log.txt\")\n",
    "    \n",
    "        try:\n",
    "            # Read the serialization log to get file paths\n",
    "            file_paths = exportable_instance.read_serialization_log()\n",
    "    \n",
    "            self.logger.info(f\"File paths from log: {file_paths}\")\n",
    "    \n",
    "            if not file_paths:\n",
    "                self.logger.error(\"No file paths found in the serialization log.\")\n",
    "                return\n",
    "    \n",
    "            # Process each file path to deserialize and inspect contents\n",
    "            for file_path in file_paths:\n",
    "                data = self.deserializer.deserialize_data(file_path)  # Use the correct method\n",
    "                \n",
    "                if isinstance(data, str) and data.startswith(\"Error\"):\n",
    "                    self.logger.error(data)  # Log errors from deserialization\n",
    "                else:\n",
    "                    # Log or print summary information about the deserialized data\n",
    "                    if isinstance(data, pd.DataFrame):\n",
    "                        self.logger.info(f\"Successfully deserialized DataFrame from {file_path}.\\n: Shape:{data.shape}. Preview: {data.head()}\")\n",
    "                    elif isinstance(data, dict):\n",
    "                        self.logger.info(f\"Successfully deserialized JSON from {file_path}.:\\n Keys:{list(data.keys())}\")\n",
    "                    else:\n",
    "                        self.logger.info(f\"Successfully deserialized data from {file_path}:\\n{data}\")  # Adjust this line if data needs special formatting\n",
    "    \n",
    "            self.logger.info(\"Report generation completed.\")\n",
    "            return True  # Indicate success\n",
    "    \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred in generate_report: {e}\")\n",
    "            return False  # Indicate failure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052b286a-95d9-4324-b835-eab3189a071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "## Testfile for SQL methods  without needing to run the GUI ##\n",
    "##############################################################\n",
    "\n",
    "'''\n",
    "Keep this test-file here to test for SQL files. Could also be deleted. \n",
    "For other methods, use the GUI, it's simpler. \n",
    "'''\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Initialize serializer, SQLHandler, and Deserialization with workspace path\n",
    "serializer = Serialization()\n",
    "deserializer = Deserialization()  # Create an instance of Deserialization\n",
    "\n",
    "## Use for example: workspace = '/home/.../Desktop'\n",
    "workspace = os.getcwd() # Just for this case, we set up the current directory\n",
    "\n",
    "sql_handler = SQLHandler(workspace, serializer)\n",
    "\n",
    "# Step 3: Read SQL Commands using the instance of SQLHandler\n",
    "\n",
    "### Use for example: file_path = '/home/.../Datasets/scriptDB2.sql'\n",
    "testdata_directory = os.path.join(os.getcwd(), 'Testdata')\n",
    "file_path = os.path.join(testdata_directory, 'scriptDB2.sql')\n",
    "sql_commands = sql_handler.read_sql(file_path)  # Call read_sql on the instance\n",
    "\n",
    "# Step 4: Execute SQL Commands\n",
    "sql_handler.execute_sql(sql_commands)\n",
    "\n",
    "# Step 5: Fetch and Print Data\n",
    "results = sql_handler.get_all_data()\n",
    "print(results)\n",
    "\n",
    "# Step 6: Serialize Data (Optional)\n",
    "# Define the filename for the serialized data\n",
    "serialized_file_name = f\"{file_path.split('/')[-1].replace('.sql', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
    "\n",
    "# Serialize the results\n",
    "serializer.serialize(results, workspace, 'pkl', filename=serialized_file_name, original_filename=file_path)\n",
    "print(f\"Data serialized to {serialized_file_name}\")\n",
    "\n",
    "# Deserialization Test\n",
    "# Attempt to deserialize the serialized file\n",
    "deserialized_data = deserializer.deserialize_data(f\"{workspace}/{serialized_file_name}\")\n",
    "print(\"Deserialized Data:\", deserialized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad799ae2-d30a-4f9c-8769-56049e5db2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import logging\n",
    "from tkinter import messagebox\n",
    "from GUImaker import GUIMaker  # Ensure this line is uncommented\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the GUI and process files.\"\"\"\n",
    "    # Initialize and run the GUI\n",
    "    try:\n",
    "        app = GUIMaker()  # Make sure GUIMaker is correctly implemented to accept necessary parameters\n",
    "        app.start()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while starting the application: {e}\")\n",
    "        messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
